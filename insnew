def generate_insights_with_outliers(user_query: str, sql_query: str):
    """
    Generate insights using top 100, random 100, last 100 rows,
    plus column metadata, statistics, and detected outliers.
    """
    try:
        # Step 1: Load latest CSV blob from Azure
        azure = AzureStorage()
        container = azure.client.get_container_client(AZURE_CONFIG["container"])
        blobs = sorted(
            container.list_blobs(name_starts_with=f"{AZURE_CONFIG['results_folder']}/"),
            key=lambda b: b.creation_time,
            reverse=True
        )
        if not blobs:
            print("No CSV blob found in storage.")
            return

        latest_blob = blobs[0]
        data = container.download_blob(latest_blob.name).readall()
        df = pd.read_csv(BytesIO(data))

        if df.empty:
            print("The dataset is empty.")
            return

        # Step 2: Generate schema metadata
        schema_summary = {
            col: {
                "type": str(df[col].dtype),
                "unique_values": int(df[col].nunique()),
                "non_null_count": int(df[col].notna().sum()),
                "null_count": int(df[col].isna().sum()),
                "top_values": df[col].value_counts().head(3).to_dict()
            }
            for col in df.columns
        }

        # Step 3: Generate statistical summary
        stats_summary = df.describe(include='all').fillna("").to_dict()

        # Step 4: Outlier detection using IQR
        outliers = {}
        numeric_cols = df.select_dtypes(include='number').columns
        for col in numeric_cols:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR
            outlier_values = df[(df[col] < lower) | (df[col] > upper)][col].tolist()
            outliers[col] = {
                "outlier_count": len(outlier_values),
                "outlier_samples": outlier_values[:5]
            }

        # Step 5: Data sampling
        top_100 = df.head(100)
        random_100 = df.sample(min(100, len(df)), random_state=42)
        last_100 = df.tail(100)

        # Step 6: Construct prompt
        prompt = f"""
You are a data analysis assistant. Based on the information below, generate a list of insights.

User Query:
{user_query}

Executed SQL Query:
{sql_query}

Column Metadata Summary:
{json.dumps(schema_summary, indent=2)}

Statistical Summary:
{json.dumps(stats_summary, indent=2)}

Detected Outliers (based on IQR method):
{json.dumps(outliers, indent=2)}

Top 100 Rows:
{top_100.to_markdown(index=False)}

Random 100 Rows:
{random_100.to_markdown(index=False)}

Last 100 Rows:
{last_100.to_markdown(index=False)}

Instructions:
Generate 5 well-structured and relevant insights that:
- Reflect trends, anomalies, or correlations in the data
- Connect clearly to the user query and SQL logic
- Include at least one insight based on the detected outliers
- Are presented in a business-relevant tone

Format:
- [Insight] â€“ [Why it matters]
"""

        # Step 7: Call OpenAI
        ai = AIService()
        response = ai.client.chat.completions.create(
            model=OPENAI_CONFIG["deployment"],
            messages=[{"role": "user", "content": prompt}],
            temperature=0.6,
            max_tokens=1000
        )

        insights = response.choices[0].message.content.strip()
        print("\nInsights Generated:\n", insights)

    except Exception as e:
        print(f"Error generating insights: {str(e)}")
